# ğŸ  House Price Prediction Using Machine Learning

## ğŸ“Œ Project Overview
This project focuses on predicting **house prices** using the **Ames Housing dataset**.  
It applies a complete **machine learning workflow**, including data cleaning, feature engineering, encoding categorical variables, model training, evaluation, and model persistence.

Multiple regression models are trained and compared to identify the best-performing approach for house price prediction.

---

## ğŸ¯ Objectives
- Perform exploratory data analysis (EDA)
- Handle missing values effectively
- Encode categorical features (Ordinal & One-Hot Encoding)
- Reduce feature complexity using cardinality analysis
- Train multiple regression models
- Evaluate models using RMSE and RÂ² score
- Save trained models for future use

---

## ğŸ“‚ Dataset
**File Name:** `AmesHousing.csv`

### Target Variable
- **SalePrice** â€“ Final sale price of the house

### Dataset Details
- Rows: 2930  
- Columns: 82 (reduced to 192 after encoding)

---

## ğŸ› ï¸ Libraries Used
- pandas  
- numpy  
- matplotlib  
- seaborn  
- scikit-learn  
- joblib  

---

## ğŸ” Exploratory Data Analysis (EDA)
- Dataset shape, columns, and summary statistics
- Missing value analysis
- Distribution of target variable (`SalePrice`)
- Categorical feature distribution
- Numerical feature relationships using:
  - Pair plots
  - Box plots
  - Scatter plots

---

## ğŸ§¹ Data Cleaning & Feature Engineering

### Handling Missing Values
- Dropped columns with **>50% missing values**
- Numerical features imputed using **median**
- Categorical features imputed using **most frequent value**

---

### Encoding Categorical Variables
- **Ordinal Encoding** applied to ordered categorical features:
  - Quality, condition, basement, garage-related features
- **Cardinality Analysis** performed to identify high-cardinality columns
- Dropped high-cardinality feature:
  - `Neighborhood`
- **One-Hot Encoding** applied to low-cardinality nominal features

---

## ğŸ§© Feature Engineering Summary
- Numerical Imputation
- Ordinal Encoding
- One-Hot Encoding
- Cardinality Reduction
- Final feature set prepared for modeling

---

## ğŸ”€ Train-Test Split
- Features (`X`) and target (`y`) separated
- Train-test split applied
- Test size: 20 samples
- Random state fixed for reproducibility

---

## ğŸ¤– Models Trained

### 1ï¸âƒ£ Linear Regression
- Baseline regression model

### 2ï¸âƒ£ Random Forest Regressor
- Ensemble-based model using multiple decision trees

### 3ï¸âƒ£ Gradient Boosting Regressor
- Boosted ensemble model for improved performance

---

## ğŸ“Š Model Evaluation

### Metrics Used
- **RMSE (Root Mean Squared Error)**
- **RÂ² Score**

### Performance Results
| Model | RMSE | RÂ² Score |
|------|------|----------|
| Linear Regression | ~14529 | ~0.943 |
| Random Forest | ~14926 | ~0.940 |
| Gradient Boosting | **~14253** | **~0.946** |

âœ… **Gradient Boosting Regressor performed best**

---

## ğŸ“ˆ Model Comparison
- RMSE comparison bar chart
- RÂ² score comparison bar chart
- Visualization confirms Gradient Boosting as the top model

---

## ğŸ’¾ Model Saving
Trained models were saved using **joblib**:
- Linear Regression model
- Random Forest Regression model
- Gradient Boosting Regression model

These models can be reused for inference without retraining.


pip install pandas numpy matplotlib seaborn scikit-learn joblib
jupyter notebook
